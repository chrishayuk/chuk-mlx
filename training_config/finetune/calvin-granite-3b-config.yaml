# Fine-tuning 
model:
  name: ibm-granite/granite-3b-code-instruct

# optimizer settings
optimizer:
  name: AdamW
  initial_lr: 1e-5
  lr_schedule:
    type: cosine_decay
    warmup_steps: 10
  betas: [0.9, 0.95]
  eps: 1e-5
  weight_decay: 0.1

# checkpointing
checkpoint:
  frequency_epochs: 1  # Checkpoint every epochs
  frequency_iterations: 500  # Checkpoint every n iterations
  output_dir: './output/checkpoints/calvin_granite_3b'

# training
training:
  num_epochs: 1
  total_iterations: 1000
  loss_function: core.models.chuk_loss_function.chukloss

# batches
batch:
  output_dir: './output/batches/calvin'
  file_prefix: 'calvin'
  pre_cache_size: 5
  shuffle: True
