# Fine-tuning 

model:
  name: ibm-granite/granite-3b-code-instruct

# optimizer settings
optimizer:
  name: AdamW
  initial_lr: 1e-5
  lr_schedule:
    type: cosine_decay
    warmup_steps: 10
  betas: [0.9, 0.95]
  eps: 1e-5
  weight_decay: 0.1

# checkpointing
checkpoint:
  frequency: 500
  output_dir: './output/calvin/checkpoints'

# training
training:
  num_epochs: 1
  total_iterations: 2000
  loss_function: models.chuk_loss_function.chukloss

# batches
batch:
  output_dir: './output/calvin/batches'
  file_prefix: 'calvin'
