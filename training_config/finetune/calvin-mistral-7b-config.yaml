# Fine-tuning 
model:
  # model name
  name: "mistralai/Mistral-7B-Instruct-v0.2"

# optimizer settings
optimizer:
  name: AdamW
  initial_lr: 1e-5  # As mentioned in the passage
  lr_schedule:
    type: cosine_decay
  betas: [0.9, 0.999]  # Standard betas for AdamW
  eps: 1e-8  # Standard epsilon for AdamW
  weight_decay: 0.01  # Typically lower

# checkpointing
checkpoint:
  frequency_epochs: 1  # Checkpoint every epochs
  frequency_iterations: 500  # Checkpoint every n iterations
  output_dir: './output/checkpoints/calvin_mistral_7b'

# training
training:
  num_epochs: 1
  total_iterations: 1000
  loss_function: core.models.chuk_loss_function.chukloss

# batches
batch:
  output_dir: './output/batches/calvin'
  file_prefix: 'calvin'
  pre_cache_size: 5
  shuffle: True