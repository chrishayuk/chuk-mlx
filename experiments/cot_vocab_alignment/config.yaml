# CoT Vocabulary Alignment Experiment
# Tests if Chain-of-Thought training creates vocabulary-aligned classifiers
name: cot_vocab_alignment
description: "Does CoT training create vocabulary classifiers like GPT-OSS?"

model: meta-llama/Llama-3.2-1B

parameters:
  num_samples: 3000
  seed: 42

  # Training settings
  max_steps: 500
  batch_size: 4
  learning_rate: 0.0002
  lora:
    rank: 16
    alpha: 32.0
    targets: [q_proj, k_proj, v_proj, o_proj]

  # Layers to check for vocabulary alignment
  check_layers_pct: [0.25, 0.5, 0.55, 0.75, 0.85, 0.95]

  # Test prompts
  test_prompts:
    - input: "7 * 8 = "
      expected: "56"
      task: multiply
    - input: "12 * 5 = "
      expected: "60"
      task: multiply
    - input: "9 * 9 = "
      expected: "81"
      task: multiply
    - input: "23 + 45 = "
      expected: "68"
      task: add
    - input: "17 + 38 = "
      expected: "55"
      task: add
    - input: "55 + 27 = "
      expected: "82"
      task: add
    - input: "89 - 34 = "
      expected: "55"
      task: subtract
    - input: "65 - 28 = "
      expected: "37"
      task: subtract
    - input: "100 - 43 = "
      expected: "57"
      task: subtract
