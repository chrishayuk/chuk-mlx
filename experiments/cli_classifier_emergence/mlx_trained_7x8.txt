Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 21236.98it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '7 x 8 = '

Tokens (7): ['<|begin_of_text|>', '7', ' x', ' ', '8', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  1.0000 ################################################## '56'
  0.0003  '560'
  0.0001  '64'
  0.0000  '112'
  0.0000  '60'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0610)
           'AAAA' (0.0069)
           ' longest' (0.0069)
           'central' (0.0050)
           'VEN' (0.0044)
  Layer  1: '-variable' (0.0078)
           'AAAA' (0.0073)
           ' Extend' (0.0044)
           '84' (0.0042)
           '76' (0.0037)
  Layer  2: ' seule' (0.0112)
           ' thirteen' (0.0068)
           ' fifteen' (0.0053)
           'anus' (0.0050)
           'umblr' (0.0050)
  Layer  3: 'PURE' (0.0092)
           'Ec' (0.0056)
           '-variable' (0.0056)
           'anus' (0.0049)
           'erald' (0.0049)
  Layer  4: '-variable' (0.0115)
           'PURE' (0.0090)
           'iske' (0.0084)
           'uppe' (0.0042)
           ' TWO' (0.0042)
  Layer  5: 'uppe' (0.0325)
           'iske' (0.0209)
           '-variable' (0.0047)
           'elijke' (0.0044)
           'elps' (0.0040)
  Layer  6: 'iske' (0.0266)
           'uppe' (0.0183)
           'elijke' (0.0172)
           'elps' (0.0067)
           ' Kushner' (0.0046)
  Layer  7: 'iske' (0.0583)
           'elijke' (0.0215)
           'ilogue' (0.0090)
           '.Bundle' (0.0074)
           '刷' (0.0070)
  Layer  8: 'iske' (0.0206)
           'ATER' (0.0056)
           '.Bundle' (0.0052)
           'šk' (0.0036)
           'elijke' (0.0036)
  Layer  9: 'ATER' (0.0100)
           'кар' (0.0047)
           'estre' (0.0047)
           'iske' (0.0047)
           'ール' (0.0042)
  Layer 10: 'OLT' (0.0223)
           'окон' (0.0068)
           'ilogue' (0.0064)
           'arde' (0.0053)
           'obl' (0.0053)
  Layer 11: '8' (0.1299)
           '八' (0.0212)
           ' Eighth' (0.0188)
           'OLT' (0.0146)
           '7' (0.0137)
  Layer 12: '-eight' (0.0593)
           '56' (0.0408)
           '48' (0.0262)
           ' Eighth' (0.0247)
           ' 八' (0.0233)
  Layer 13: '56' (0.8047)
           '48' (0.0850)
           '52' (0.0115)
           '58' (0.0101)
           '64' (0.0089)
  Layer 14: '56' (0.9922)
           '64' (0.0026)
           '52' (0.0010)
           '560' (0.0000)
           '58' (0.0000)
  Layer 15: '56' (1.0000)
           '560' (0.0003)
           '64' (0.0001)
           '112' (0.0000)
           '60' (0.0000)
