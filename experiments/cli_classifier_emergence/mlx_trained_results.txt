=== POST-TRAINING ANALYSIS (mlx-lm trained, 500 steps) ===

--- Prompt: '7 x 8 = ' ---
Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 18265.89it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '7 x 8 = '

Tokens (7): ['<|begin_of_text|>', '7', ' x', ' ', '8', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  1.0000 ################################################## '56'
  0.0003  '560'
  0.0001  '64'
  0.0000  '112'
  0.0000  '60'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0610)
           'AAAA' (0.0069)
           ' longest' (0.0069)
           'central' (0.0050)
           'VEN' (0.0044)
  Layer  1: '-variable' (0.0078)
           'AAAA' (0.0073)
           ' Extend' (0.0044)
           '84' (0.0042)
           '76' (0.0037)
  Layer  2: ' seule' (0.0112)
           ' thirteen' (0.0068)
           ' fifteen' (0.0053)
           'anus' (0.0050)
           'umblr' (0.0050)
  Layer  3: 'PURE' (0.0092)
           'Ec' (0.0056)
           '-variable' (0.0056)
           'anus' (0.0049)
           'erald' (0.0049)
  Layer  4: '-variable' (0.0115)
           'PURE' (0.0090)
           'iske' (0.0084)
           'uppe' (0.0042)
           ' TWO' (0.0042)
  Layer  5: 'uppe' (0.0325)
           'iske' (0.0209)
           '-variable' (0.0047)
           'elijke' (0.0044)
           'elps' (0.0040)
  Layer  6: 'iske' (0.0266)
           'uppe' (0.0183)
           'elijke' (0.0172)
           'elps' (0.0067)
           ' Kushner' (0.0046)
  Layer  7: 'iske' (0.0583)
           'elijke' (0.0215)
           'ilogue' (0.0090)
           '.Bundle' (0.0074)
           '刷' (0.0070)
  Layer  8: 'iske' (0.0206)
           'ATER' (0.0056)
           '.Bundle' (0.0052)
           'šk' (0.0036)
           'elijke' (0.0036)
  Layer  9: 'ATER' (0.0100)
           'кар' (0.0047)
           'estre' (0.0047)
           'iske' (0.0047)
           'ール' (0.0042)
  Layer 10: 'OLT' (0.0223)
           'окон' (0.0068)
           'ilogue' (0.0064)
           'arde' (0.0053)
           'obl' (0.0053)
  Layer 11: '8' (0.1299)
           '八' (0.0212)
           ' Eighth' (0.0188)
           'OLT' (0.0146)
           '7' (0.0137)
  Layer 12: '-eight' (0.0593)
           '56' (0.0408)
           '48' (0.0262)
           ' Eighth' (0.0247)
           ' 八' (0.0233)
  Layer 13: '56' (0.8047)
           '48' (0.0850)
           '52' (0.0115)
           '58' (0.0101)
           '64' (0.0089)
  Layer 14: '56' (0.9922)
           '64' (0.0026)
           '52' (0.0010)
           '560' (0.0000)
           '58' (0.0000)
  Layer 15: '56' (1.0000)
           '560' (0.0003)
           '64' (0.0001)
           '112' (0.0000)
           '60' (0.0000)

--- Prompt: '12 x 5 = ' ---
Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 15040.09it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '12 x 5 = '

Tokens (7): ['<|begin_of_text|>', '12', ' x', ' ', '5', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  1.0000 ################################################## '60'
  0.0001  '50'
  0.0000  '62'
  0.0000  '55'
  0.0000  '30'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0500)
           'AAAA' (0.0072)
           ' longest' (0.0068)
           'central' (0.0046)
           'VEN' (0.0046)
  Layer  1: 'AAAA' (0.0058)
           ' zero' (0.0043)
           'GR' (0.0040)
           '84' (0.0040)
           'lixir' (0.0036)
  Layer  2: ' seule' (0.0168)
           'umblr' (0.0079)
           ' foe' (0.0070)
           ' thirteen' (0.0066)
           'five' (0.0058)
  Layer  3: ' Royale' (0.0092)
           ' seule' (0.0052)
           'PURE' (0.0052)
           'iske' (0.0052)
           'five' (0.0043)
  Layer  4: 'iske' (0.0292)
           'PURE' (0.0089)
           ' SIX' (0.0074)
           ' TWO' (0.0074)
           ' PURE' (0.0065)
  Layer  5: 'iske' (0.0247)
           'uppe' (0.0217)
           ' SIX' (0.0067)
           'elps' (0.0049)
           '四' (0.0039)
  Layer  6: 'iske' (0.0444)
           'uppe' (0.0164)
           'elps' (0.0106)
           'elijke' (0.0072)
           'ilogue' (0.0035)
  Layer  7: 'iske' (0.0815)
           'ATER' (0.0205)
           'elijke' (0.0161)
           'elps' (0.0110)
           'uppe' (0.0076)
  Layer  8: 'ATER' (0.0168)
           'iske' (0.0157)
           'šk' (0.0051)
           '.Bundle' (0.0048)
           'PLY' (0.0045)
  Layer  9: 'ATER' (0.0157)
           'šk' (0.0054)
           'ATAL' (0.0054)
           '.Bundle' (0.0054)
           'iske' (0.0051)
  Layer 10: '-total' (0.0161)
           ' total' (0.0161)
           'OLT' (0.0125)
           '脂' (0.0104)
           'ihat' (0.0098)
  Layer 11: '12' (0.0986)
           '十二' (0.0342)
           'OLT' (0.0283)
           'izi' (0.0208)
           '124' (0.0208)
  Layer 12: '60' (0.0840)
           '50' (0.0508)
           ' Fifty' (0.0155)
           'OLT' (0.0146)
           ' fifty' (0.0101)
  Layer 13: '60' (0.4922)
           '50' (0.4336)
           '55' (0.0216)
           '65' (0.0070)
           '625' (0.0038)
  Layer 14: '60' (1.0000) <- peak
           '50' (0.0015)
           '62' (0.0002)
           ' sixty' (0.0001)
           '55' (0.0001)
  Layer 15: '60' (1.0000)
           '50' (0.0001)
           '62' (0.0000)
           '55' (0.0000)
           '30' (0.0000)

--- Prompt: '45 x 45 = ' ---
Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 33893.37it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '45 x 45 = '

Tokens (7): ['<|begin_of_text|>', '45', ' x', ' ', '45', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  0.9766 ################################################ '90'
  0.0058  '180'
  0.0045  '190'
  0.0040  '140'
  0.0019  '80'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0923)
           'AAAA' (0.0067)
           ' longest' (0.0063)
           ' zero' (0.0041)
           'VEN' (0.0038)
  Layer  1: ' cheat' (0.0126)
           ' constr' (0.0092)
           ' crushers' (0.0087)
           'general' (0.0087)
           ' zero' (0.0087)
  Layer  2: ' foe' (0.0099)
           'PLICIT' (0.0056)
           'umblr' (0.0053)
           '124' (0.0053)
           ' crushers' (0.0047)
  Layer  3: 'تش' (0.0098)
           'iske' (0.0071)
           'Duplicates' (0.0052)
           ' ZERO' (0.0049)
           ' foe' (0.0043)
  Layer  4: 'iske' (0.0234)
           'uppe' (0.0076)
           ' TWO' (0.0067)
           '★★' (0.0038)
           '124' (0.0035)
  Layer  5: 'uppe' (0.0386)
           'iske' (0.0182)
           'posted' (0.0059)
           'elps' (0.0043)
           ' SIX' (0.0032)
  Layer  6: 'iske' (0.0283)
           'uppe' (0.0250)
           'elps' (0.0092)
           'elijke' (0.0060)
           ' produit' (0.0052)
  Layer  7: 'iske' (0.0688)
           'ATER' (0.0164)
           'elijke' (0.0127)
           '.Bundle' (0.0120)
           'elps' (0.0087)
  Layer  8: 'iske' (0.0270)
           'ATER' (0.0093)
           'ále' (0.0060)
           '.Bundle' (0.0044)
           'àn' (0.0042)
  Layer  9: 'ATER' (0.0117)
           'iske' (0.0075)
           'кар' (0.0049)
           'ATAL' (0.0046)
           'šk' (0.0043)
  Layer 10: 'upt' (0.0082)
           'ATAL' (0.0072)
           'URATION' (0.0068)
           'ール' (0.0060)
           'udent' (0.0060)
  Layer 11: 'iple' (0.0330)
           '50' (0.0256)
           'ovit' (0.0240)
           'idata' (0.0095)
           'ewood' (0.0095)
  Layer 12: 'idata' (0.0347)
           '60' (0.0347)
           'ovit' (0.0106)
           'افت' (0.0106)
           'ewood' (0.0077)
  Layer 13: '60' (0.1797)
           '75' (0.0796)
           '150' (0.0747)
           '90' (0.0400)
           '180' (0.0378)
  Layer 14: '180' (0.4043)
           '90' (0.2773)
           '150' (0.1396)
           '140' (0.0157)
           '190' (0.0084)
  Layer 15: '90' (0.9766)
           '180' (0.0058)
           '190' (0.0045)
           '140' (0.0040)
           '80' (0.0019)

--- Prompt: '23 + 45 = ' ---
Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 23205.00it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '23 + 45 = '

Tokens (7): ['<|begin_of_text|>', '23', ' +', ' ', '45', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  1.0000 ################################################## '68'
  0.0001  '70'
  0.0001  '67'
  0.0000  '58'
  0.0000  '66'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0889)
           'AAAA' (0.0069)
           ' longest' (0.0060)
           ' zero' (0.0047)
           'VEN' (0.0034)
  Layer  1: ' cheat' (0.0128)
           ' zero' (0.0099)
           ' GENERAL' (0.0088)
           'general' (0.0088)
           ' crushers' (0.0078)
  Layer  2: ' foe' (0.0121)
           ' ZERO' (0.0069)
           'umblr' (0.0061)
           'PLICIT' (0.0057)
           ' zero' (0.0057)
  Layer  3: 'تش' (0.0082)
           ' ZERO' (0.0077)
           ' foe' (0.0060)
           ' TWO' (0.0056)
           'iske' (0.0053)
  Layer  4: 'iske' (0.0132)
           'uppe' (0.0103)
           ' TWO' (0.0071)
           '★★' (0.0049)
           ' CROSS' (0.0038)
  Layer  5: 'elps' (0.0223)
           '四' (0.0072)
           'uppe' (0.0072)
           'iske' (0.0072)
           'elijke' (0.0056)
  Layer  6: 'uppe' (0.0300)
           'elps' (0.0249)
           'elijke' (0.0110)
           'orama' (0.0098)
           '-rounded' (0.0049)
  Layer  7: 'elijke' (0.0204)
           'ATER' (0.0204)
           'elps' (0.0109)
           'iske' (0.0096)
           '.Bundle' (0.0085)
  Layer  8: 'itra' (0.0096)
           'ATER' (0.0085)
           'PLY' (0.0062)
           '.Bundle' (0.0058)
           'asion' (0.0045)
  Layer  9: '-total' (0.0101)
           '.Bundle' (0.0074)
           'يه' (0.0045)
           'later' (0.0042)
           'ła' (0.0037)
  Layer 10: ' mortar' (0.0093)
           '-total' (0.0068)
           'เขา' (0.0044)
           'iała' (0.0044)
           'etti' (0.0044)
  Layer 11: 'วด' (0.0159)
           ' bucks' (0.0159)
           'kom' (0.0159)
           'itte' (0.0096)
           'arde' (0.0080)
  Layer 12: 'иск' (0.0090)
           'uur' (0.0075)
           ' WORLD' (0.0062)
           'uda' (0.0062)
           'idata' (0.0058)
  Layer 13: '68' (0.1436)
           '70' (0.0679)
           '60' (0.0564)
           '69' (0.0413)
           '74' (0.0361)
  Layer 14: '68' (0.7383)
           '58' (0.1133)
           '62' (0.0269)
           '66' (0.0222)
           '67' (0.0197)
  Layer 15: '68' (1.0000)
           '70' (0.0001)
           '67' (0.0001)
           '58' (0.0000)
           '66' (0.0000)

--- Prompt: '17 + 38 = ' ---
Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 52265.47it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '17 + 38 = '

Tokens (7): ['<|begin_of_text|>', '17', ' +', ' ', '38', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  1.0000 ################################################## '55'
  0.0006  '155'
  0.0001  '51'
  0.0001  '105'
  0.0001  '53'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0688)
           'AAAA' (0.0056)
           ' longest' (0.0050)
           ' zero' (0.0050)
           ' Butter' (0.0030)
  Layer  1: ' GENERAL' (0.0144)
           ' zero' (0.0144)
           'general' (0.0105)
           ' cheat' (0.0082)
           '.bulk' (0.0073)
  Layer  2: 'PLICIT' (0.0153)
           ' foe' (0.0105)
           'umblr' (0.0082)
           ' sacrific' (0.0046)
           ' zero' (0.0044)
  Layer  3: ' TWO' (0.0071)
           'umblr' (0.0059)
           'uppe' (0.0041)
           ' foe' (0.0041)
           '★★' (0.0041)
  Layer  4: 'uppe' (0.0165)
           ' TWO' (0.0094)
           'elijke' (0.0078)
           'iske' (0.0060)
           '★★' (0.0050)
  Layer  5: 'elps' (0.0255)
           'uppe' (0.0137)
           'elijke' (0.0113)
           '四' (0.0093)
           '116' (0.0047)
  Layer  6: 'uppe' (0.0515)
           'elps' (0.0276)
           'elijke' (0.0228)
           ' produit' (0.0079)
           'orama' (0.0058)
  Layer  7: 'elijke' (0.0310)
           'ATER' (0.0146)
           'elps' (0.0107)
           '.Bundle' (0.0107)
           'uppe' (0.0095)
  Layer  8: 'itra' (0.0084)
           'ATER' (0.0074)
           '.Bundle' (0.0074)
           'PLY' (0.0062)
           'later' (0.0048)
  Layer  9: '.Bundle' (0.0089)
           '-total' (0.0084)
           'later' (0.0051)
           'يه' (0.0048)
           'ła' (0.0042)
  Layer 10: ' mortar' (0.0091)
           '-total' (0.0059)
           'iała' (0.0046)
           '.Bundle' (0.0046)
           'เขา' (0.0043)
  Layer 11: 'itre' (0.0115)
           '่างก' (0.0108)
           'pert' (0.0079)
           ' INSTANCE' (0.0066)
           ' seventeen' (0.0062)
  Layer 12: 'legt' (0.0221)
           ' cuz' (0.0076)
           'ayan' (0.0063)
           '17' (0.0056)
           ' butcher' (0.0052)
  Layer 13: '37' (0.1475)
           '59' (0.1079)
           '57' (0.0542)
           '53' (0.0349)
           '27' (0.0327)
  Layer 14: '55' (0.7852)
           '51' (0.0605)
           '53' (0.0442)
           '54' (0.0369)
           '59' (0.0253)
  Layer 15: '55' (1.0000)
           '155' (0.0006)
           '51' (0.0001)
           '105' (0.0001)
           '53' (0.0001)

--- Prompt: '89 - 34 = ' ---
Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 14873.42it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '89 - 34 = '

Tokens (7): ['<|begin_of_text|>', '89', ' -', ' ', '34', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  1.0000 ################################################## '55'
  0.0017  '45'
  0.0006  '56'
  0.0005  '54'
  0.0002  '53'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0781)
           'AAAA' (0.0073)
           ' longest' (0.0056)
           'VEN' (0.0039)
           ' zero' (0.0039)
  Layer  1: ' GENERAL' (0.0080)
           'AAAA' (0.0080)
           '-variable' (0.0070)
           ' zero' (0.0070)
           'general' (0.0062)
  Layer  2: 'PLICIT' (0.0166)
           'umblr' (0.0084)
           ' foe' (0.0057)
           ' cheat' (0.0051)
           ' seule' (0.0048)
  Layer  3: 'iske' (0.0067)
           'تش' (0.0059)
           'umblr' (0.0055)
           'PLICIT' (0.0040)
           ' seule' (0.0034)
  Layer  4: 'iske' (0.0076)
           'uppe' (0.0067)
           ' SIX' (0.0055)
           'elijke' (0.0042)
           ' TWO' (0.0039)
  Layer  5: 'iske' (0.0140)
           'elijke' (0.0109)
           'uppe' (0.0096)
           ' SIX' (0.0055)
           'elps' (0.0043)
  Layer  6: 'elijke' (0.0195)
           'elps' (0.0134)
           'uppe' (0.0111)
           'iske' (0.0067)
           'ONLY' (0.0067)
  Layer  7: 'elijke' (0.0227)
           'iske' (0.0114)
           'ATER' (0.0101)
           '.Bundle' (0.0084)
           'ooter' (0.0057)
  Layer  8: ' fewer' (0.0062)
           'arde' (0.0055)
           'URE' (0.0055)
           'اک' (0.0049)
           ' disarm' (0.0049)
  Layer  9: 'arde' (0.0105)
           ' fewer' (0.0093)
           'ouden' (0.0082)
           'ато' (0.0072)
           'ninger' (0.0063)
  Layer 10: 'remaining' (0.5391)
           ' remaining' (0.2539)
           ' remainder' (0.1064)
           ' Remaining' (0.0569)
           'Remaining' (0.0112)
  Layer 11: 'isté' (0.0913)
           'istrat' (0.0096)
           'istra' (0.0085)
           '79' (0.0085)
           'ular' (0.0070)
  Layer 12: 'isté' (0.0187)
           'ILLS' (0.0120)
           ' scratch' (0.0073)
           'иск' (0.0060)
           '77' (0.0060)
  Layer 13: '55' (0.2021)
           '59' (0.1299)
           '53' (0.0957)
           '65' (0.0374)
           '54' (0.0352)
  Layer 14: '55' (0.8320)
           '53' (0.0564)
           '54' (0.0500)
           '56' (0.0284)
           '45' (0.0104)
  Layer 15: '55' (1.0000)
           '45' (0.0017)
           '56' (0.0006)
           '54' (0.0005)
           '53' (0.0002)

--- Prompt: '65 - 28 = ' ---
Loading model: meta-llama/Llama-3.2-1B + adapter: experiments/cli_classifier_emergence/mlx_checkpoint
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 40622.80it/s]
Model: meta-llama/Llama-3.2-1B
  Layers: 16
  Hidden size: 2048
  Vocab size: 128000
  Tied embeddings: False
  Mode: RAW (model has no chat template)

Note: Prompt has trailing whitespace which affects tokenization
  This changes what the model predicts (next token after space vs after last word)
  For arithmetic prompts like 'X + Y = ', trailing space often helps get answers

Analyzing: '65 - 28 = '

Tokens (7): ['<|begin_of_text|>', '65', ' -', ' ', '28', ' =', ' ']
Captured layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

=== Final Prediction ===
  1.0000 ################################################## '37'
  0.0008  '36'
  0.0007  '35'
  0.0002  '39'
  0.0002  '38'

=== Logit Lens (top-5 at each layer) ===
  Layer  0: ' ' (0.0786)
           'AAAA' (0.0073)
           ' longest' (0.0069)
           ' zero' (0.0044)
           'VEN' (0.0039)
  Layer  1: ' zero' (0.0194)
           '.bulk' (0.0142)
           'general' (0.0104)
           'PLICIT' (0.0081)
           ' GENERAL' (0.0076)
  Layer  2: 'umblr' (0.0136)
           'PLICIT' (0.0120)
           ' foe' (0.0082)
           ' zero' (0.0057)
           ' ZERO' (0.0053)
  Layer  3: 'umblr' (0.0071)
           ' ZERO' (0.0059)
           'تش' (0.0056)
           '-opening' (0.0056)
           ' SIX' (0.0052)
  Layer  4: 'uppe' (0.0141)
           ' SIX' (0.0085)
           'PURE' (0.0055)
           'iske' (0.0055)
           'elijke' (0.0052)
  Layer  5: 'uppe' (0.0261)
           'iske' (0.0132)
           'elijke' (0.0103)
           ' SIX' (0.0075)
           'elps' (0.0055)
  Layer  6: 'uppe' (0.0240)
           'elijke' (0.0199)
           'elps' (0.0187)
           'iske' (0.0065)
           '十八' (0.0061)
  Layer  7: 'elijke' (0.0243)
           'iske' (0.0139)
           '.Bundle' (0.0061)
           'uppe' (0.0054)
           'ATER' (0.0048)
  Layer  8: ' disarm' (0.0068)
           ' fewer' (0.0060)
           'اک' (0.0047)
           'arde' (0.0047)
           'URE' (0.0047)
  Layer  9: 'arde' (0.0115)
           'ớt' (0.0084)
           'ато' (0.0074)
           'ouden' (0.0074)
           ' fewer' (0.0070)
  Layer 10: 'remaining' (0.4629)
           ' remaining' (0.2793)
           ' remainder' (0.1328)
           ' Remaining' (0.0708)
           'Remaining' (0.0109)
  Layer 11: 'isté' (0.0178)
           '68' (0.0084)
           '77' (0.0079)
           'ByExample' (0.0074)
           'fell' (0.0074)
  Layer 12: '77' (0.0413)
           '59' (0.0208)
           '67' (0.0134)
           '69' (0.0098)
           '37' (0.0092)
  Layer 13: '59' (0.1074)
           '65' (0.0574)
           '37' (0.0574)
           'phys' (0.0447)
           '53' (0.0422)
  Layer 14: '37' (0.5938)
           '27' (0.0356)
           '43' (0.0278)
           '35' (0.0245)
           '47' (0.0148)
  Layer 15: '37' (1.0000)
           '36' (0.0008)
           '35' (0.0007)
           '39' (0.0002)
           '38' (0.0002)
