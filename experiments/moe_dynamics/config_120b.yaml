# GPT-OSS-120B MoE Dynamics Experiment Configuration
# Architecture: 36 layers, 128 experts/layer, k=4 routing
# Total experts: 4,608 (6x more than 20B's 768)

model: "openai/gpt-oss-120b"

# Architecture details (for reference)
architecture:
  num_hidden_layers: 36
  num_local_experts: 128
  num_experts_per_tok: 4
  hidden_size: 2880
  intermediate_size: 2880
  total_experts: 4608  # 36 * 128

# Analyses to run
analyses:
  - cold_experts
  - circuits
  - interference
  - merging
  # - generation  # Optional, slower
  # - attention_routing  # Optional
  # - task_aware  # Optional

# Cold expert analysis configuration
cold_experts:
  threshold: 0.01  # Experts activated <1% are "cold"
  num_prompts: 50  # Reduced for memory (120B is large)

# Circuit analysis configuration
circuits:
  correlation_threshold: 0.3
  num_prompts: 30
  min_circuit_length: 5  # Higher for 36 layers

# Expert interference configuration
interference:
  k_values: [1, 2, 4]
  num_prompts: 20

# Expert merging configuration
merging:
  similarity_threshold: 0.8
  sample_layers: [0, 9, 18, 27, 35]  # Spread across 36 layers

# Generation dynamics (optional - memory intensive)
generation:
  max_tokens: 30
  track_layers: [0, 9, 18, 27, 35]
  num_prompts: 10

# Task-aware prediction
task_aware:
  probe_layer: 6  # ~1/6 through the network
  prediction_targets: [12, 18, 24, 30]
  train_prompts: 50

# Prompts for analysis (same as 20B for comparison)
prompts:
  math:
    - "127 * 89 = "
    - "456 + 789 = "
    - "sqrt(144) = "
    - "What is 15% of 200?"
    - "Calculate 2^10"

  code:
    - "def fibonacci(n):"
    - "import numpy as np"
    - "class DataProcessor:"
    - "for item in items:"
    - "async def fetch_data():"

  language:
    - "The capital of France is"
    - "Shakespeare wrote many"
    - "The opposite of hot is"
    - "In the beginning there was"
    - "To be or not to be"

  reasoning:
    - "If all cats are mammals, then"
    - "To solve this equation, first"
    - "Therefore, we can conclude that"
    - "The logical consequence is"
    - "Given that A implies B,"

  general:
    - "Once upon a time"
    - "The quick brown fox"
    - "Hello, my name is"
    - "To summarize the main points"
    - "In conclusion,"

# Lite model configuration (tiered allocation for 36 layers, 128 experts)
# Based on 20B findings: early/late layers need fewer experts
lite_config:
  # Conservative estimates - may need adjustment after analysis
  early:
    layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  # L0-L11 (1/3)
    keep: 32  # 25% of 128 experts (vs 50% in 20B)

  middle:
    layers: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]  # L12-L23 (1/3)
    keep: 48  # 37.5% of 128 experts (vs 62.5% in 20B)

  late:
    layers: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]  # L24-L35 (1/3)
    keep: 32  # 25% of 128 experts (vs 50% in 20B)

  # Total: 12*32 + 12*48 + 12*32 = 384 + 576 + 384 = 1,344 experts
  # Reduction: 1,344 / 4,608 = 29% retained = 71% reduction (conservative)

  # Aggressive variant (matching 20B's 92% reduction)
  aggressive:
    early:
      layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
      keep: 8  # 4 teams of 2 experts
    middle:
      layers: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
      keep: 16  # 8 teams of 2 experts
    late:
      layers: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
      keep: 12  # 6 teams of 2 experts
    # Total: 12*8 + 12*16 + 12*12 = 96 + 192 + 144 = 432 experts
    # Reduction: 432 / 4,608 = 9.4% retained = 90.6% reduction

# Output configuration
output:
  results_dir: "./results/120b"
  lite_model_dir: "./gpt-oss-120b-lite"
