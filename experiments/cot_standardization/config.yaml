# Standardized CoT Training Configuration

experiment:
  name: cot_standardization
  description: Train standardized CoT format for virtual expert routing

# Model options
model:
  # Primary target - MUST use Chat variant for native chat template
  name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  # Alternatives (ensure they have chat templates):
  # name: HuggingFaceTB/SmolLM-1.7B-Instruct
  # name: meta-llama/Llama-3.2-1B-Instruct

# Training parameters
training:
  # SFT phase
  sft:
    epochs: 10
    learning_rate: 2e-5
    batch_size: 4

  # RL phase
  rl:
    iterations: 5
    learning_rate: 5e-7
    batch_size: 8
    baseline_decay: 0.9
    temperature: 0.7

# Model fine-tuning
fine_tuning:
  # Unfreeze last N layers + lm_head (6 needed for format learning)
  unfreeze_layers: 6
  max_seq_len: 384

# Data generation
data:
  # 100 per class = 450 total (recommended)
  examples_per_class: 100

  # Expert classes
  classes:
    - math           # multiply, add, subtract, divide
    - word_problem   # GSM8K-style
    - csp            # scheduling
    - time           # timezone queries
    - none           # passthrough

# Evaluation
evaluation:
  # Reward thresholds
  correct_threshold: 0.7

  # Test robustness with extra instructions
  robustness_test: true

# Format specification
format:
  # Pattern: <expert>: <spec> -> <result>
  separator: ":"
  arrow: "->"

  # Word problem format
  word_problem:
    entity_separator: ":"
    ops_separator: "|"

  # CSP format
  csp:
    task_separator: ","
    constraint_separator: "|"

# Output
output:
  results_dir: results
  save_model: false  # Set to true to save trained model
