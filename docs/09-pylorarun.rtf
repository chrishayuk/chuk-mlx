{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Calvin Scale Dataset\
\
1000 iterations\
Batch Size of 2\
8 Lora Layers\
Mistral 0.2\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
chrishayuk$ python lora.py --train --model mistralai/Mistral-7B-Instruct-v0.2 --data /Users/christopherhay/chris-source/chuk-datasets/datasets/calvin_scale/output/calvin_scale_llama/ --batch-size 2 --lora-layers 8 --iters 1000\
Loading pretrained model\
Fetching 11 files: 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 11/11 [00:00<00:00, 93966.08it/s]\
Total parameters 7242.584M\
Trainable parameters 0.852M\
Loading datasets\
Training\
Iter 1: Val loss 4.817, Val took 9.276s\
Iter 10: Train loss 5.027, It/sec 1.497, Tokens/sec 318.950\
Iter 20: Train loss 4.201, It/sec 2.254, Tokens/sec 295.303\
Iter 30: Train loss 2.580, It/sec 1.691, Tokens/sec 294.789\
Iter 40: Train loss 2.397, It/sec 2.293, Tokens/sec 290.296\
Iter 50: Train loss 1.121, It/sec 0.939, Tokens/sec 297.056\
Iter 60: Train loss 1.585, It/sec 1.741, Tokens/sec 266.391\
Iter 70: Train loss 1.237, It/sec 1.773, Tokens/sec 309.837\
Iter 80: Train loss 0.967, It/sec 1.418, Tokens/sec 311.240\
Iter 90: Train loss 1.094, It/sec 1.653, Tokens/sec 281.945\
Iter 100: Train loss 0.824, It/sec 1.563, Tokens/sec 275.307\
Iter 100: Saved adapter weights to adapters.npz.\
Iter 110: Train loss 0.889, It/sec 1.292, Tokens/sec 247.145\
Iter 120: Train loss 0.704, It/sec 1.178, Tokens/sec 249.193\
Iter 130: Train loss 0.959, It/sec 1.886, Tokens/sec 277.051\
Iter 140: Train loss 0.826, It/sec 1.592, Tokens/sec 269.406\
Iter 150: Train loss 1.087, It/sec 2.736, Tokens/sec 257.202\
Iter 160: Train loss 0.778, It/sec 1.490, Tokens/sec 279.933\
Iter 170: Train loss 0.757, It/sec 1.391, Tokens/sec 256.876\
Iter 180: Train loss 0.899, It/sec 2.127, Tokens/sec 264.002\
Iter 190: Train loss 0.675, It/sec 1.310, Tokens/sec 286.050\
Iter 200: Train loss 0.708, It/sec 1.967, Tokens/sec 305.746\
Iter 200: Val loss 0.486, Val took 10.591s\
Iter 200: Saved adapter weights to adapters.npz.\
Iter 210: Train loss 0.922, It/sec 2.207, Tokens/sec 267.661\
Iter 220: Train loss 1.013, It/sec 2.840, Tokens/sec 265.514\
Iter 230: Train loss 0.669, It/sec 1.668, Tokens/sec 327.465\
Iter 240: Train loss 0.800, It/sec 2.379, Tokens/sec 302.819\
Iter 250: Train loss 0.785, It/sec 1.844, Tokens/sec 261.693\
Iter 260: Train loss 0.627, It/sec 1.361, Tokens/sec 261.797\
Iter 270: Train loss 0.644, It/sec 1.243, Tokens/sec 263.387\
Iter 280: Train loss 0.565, It/sec 1.298, Tokens/sec 281.826\
Iter 290: Train loss 0.697, It/sec 1.641, Tokens/sec 276.427\
Iter 300: Train loss 0.804, It/sec 1.789, Tokens/sec 259.379\
Iter 300: Saved adapter weights to adapters.npz.\
Iter 310: Train loss 0.741, It/sec 1.665, Tokens/sec 352.241\
Iter 320: Train loss 0.557, It/sec 1.354, Tokens/sec 294.725\
Iter 330: Train loss 0.882, It/sec 2.156, Tokens/sec 258.095\
Iter 340: Train loss 0.601, It/sec 1.591, Tokens/sec 311.902\
Iter 350: Train loss 0.837, It/sec 2.207, Tokens/sec 268.096\
Iter 360: Train loss 0.806, It/sec 2.623, Tokens/sec 283.799\
Iter 370: Train loss 0.735, It/sec 1.822, Tokens/sec 261.942\
Iter 380: Train loss 0.447, It/sec 1.123, Tokens/sec 272.229\
Iter 390: Train loss 0.783, It/sec 2.301, Tokens/sec 340.576\
Iter 400: Train loss 0.508, It/sec 1.242, Tokens/sec 268.936\
Iter 400: Val loss 0.441, Val took 10.511s\
Iter 400: Saved adapter weights to adapters.npz.\
Iter 410: Train loss 0.682, It/sec 1.807, Tokens/sec 282.030\
Iter 420: Train loss 0.640, It/sec 1.389, Tokens/sec 260.828\
Iter 430: Train loss 0.653, It/sec 1.555, Tokens/sec 268.831\
Iter 440: Train loss 0.664, It/sec 1.445, Tokens/sec 273.166\
Iter 450: Train loss 0.438, It/sec 1.126, Tokens/sec 298.624\
Iter 460: Train loss 0.702, It/sec 1.845, Tokens/sec 316.964\
Iter 470: Train loss 0.687, It/sec 1.793, Tokens/sec 311.516\
Iter 480: Train loss 0.923, It/sec 2.083, Tokens/sec 258.648\
Iter 490: Train loss 0.845, It/sec 2.234, Tokens/sec 263.119\
Iter 500: Train loss 0.634, It/sec 1.598, Tokens/sec 314.243\
Iter 500: Saved adapter weights to adapters.npz.\
Iter 510: Train loss 0.603, It/sec 1.797, Tokens/sec 319.045\
Iter 520: Train loss 0.836, It/sec 2.168, Tokens/sec 314.746\
Iter 530: Train loss 0.730, It/sec 2.120, Tokens/sec 269.285\
Iter 540: Train loss 0.718, It/sec 2.090, Tokens/sec 266.225\
Iter 550: Train loss 0.638, It/sec 1.520, Tokens/sec 257.256\
Iter 560: Train loss 0.573, It/sec 1.495, Tokens/sec 331.840\
Iter 570: Train loss 0.576, It/sec 1.479, Tokens/sec 253.453\
Iter 580: Train loss 0.691, It/sec 1.698, Tokens/sec 290.102\
Iter 590: Train loss 0.491, It/sec 1.027, Tokens/sec 235.982\
Iter 600: Train loss 0.525, It/sec 1.342, Tokens/sec 315.340\
Iter 600: Val loss 0.413, Val took 10.293s\
Iter 600: Saved adapter weights to adapters.npz.\
Iter 610: Train loss 0.632, It/sec 1.318, Tokens/sec 263.274\
Iter 620: Train loss 0.601, It/sec 1.687, Tokens/sec 287.010\
Iter 630: Train loss 0.542, It/sec 1.529, Tokens/sec 340.686\
Iter 640: Train loss 0.641, It/sec 1.614, Tokens/sec 307.049\
Iter 650: Train loss 0.639, It/sec 1.512, Tokens/sec 284.671\
Iter 660: Train loss 0.557, It/sec 1.462, Tokens/sec 256.323\
Iter 670: Train loss 0.711, It/sec 2.113, Tokens/sec 266.880\
Iter 680: Train loss 0.391, It/sec 1.061, Tokens/sec 260.417\
Iter 690: Train loss 0.765, It/sec 2.578, Tokens/sec 260.411\
Iter 700: Train loss 0.630, It/sec 1.813, Tokens/sec 275.057\
Iter 700: Saved adapter weights to adapters.npz.\
Iter 710: Train loss 0.701, It/sec 2.112, Tokens/sec 264.167\
Iter 720: Train loss 0.528, It/sec 1.403, Tokens/sec 277.564\
Iter 730: Train loss 0.657, It/sec 1.871, Tokens/sec 268.986\
Iter 740: Train loss 0.626, It/sec 1.853, Tokens/sec 278.104\
Iter 750: Train loss 0.515, It/sec 1.613, Tokens/sec 320.541\
Iter 760: Train loss 0.637, It/sec 1.878, Tokens/sec 275.663\
Iter 770: Train loss 0.652, It/sec 1.829, Tokens/sec 267.234\
Iter 780: Train loss 0.720, It/sec 2.123, Tokens/sec 306.626\
Iter 790: Train loss 0.551, It/sec 1.360, Tokens/sec 257.406\
Iter 800: Train loss 0.645, It/sec 1.465, Tokens/sec 243.346\
Iter 800: Val loss 0.371, Val took 10.581s\
Iter 800: Saved adapter weights to adapters.npz.\
Iter 810: Train loss 0.748, It/sec 2.524, Tokens/sec 253.707\
Iter 820: Train loss 0.512, It/sec 1.467, Tokens/sec 256.528\
Iter 830: Train loss 0.569, It/sec 1.297, Tokens/sec 252.969\
Iter 840: Train loss 0.646, It/sec 1.447, Tokens/sec 275.376\
Iter 850: Train loss 0.651, It/sec 1.893, Tokens/sec 236.638\
Iter 860: Train loss 0.553, It/sec 1.446, Tokens/sec 246.351\
Iter 870: Train loss 0.623, It/sec 1.476, Tokens/sec 244.747\
Iter 880: Train loss 0.719, It/sec 2.417, Tokens/sec 235.645\
Iter 890: Train loss 0.566, It/sec 1.652, Tokens/sec 287.230\
Iter 900: Train loss 0.506, It/sec 1.386, Tokens/sec 240.148\
Iter 900: Saved adapter weights to adapters.npz.\
Iter 910: Train loss 0.610, It/sec 1.873, Tokens/sec 245.217\
Iter 920: Train loss 0.583, It/sec 1.504, Tokens/sec 219.783\
Iter 930: Train loss 0.556, It/sec 1.253, Tokens/sec 213.812\
Iter 940: Train loss 0.567, It/sec 1.004, Tokens/sec 226.655\
Iter 950: Train loss 0.479, It/sec 0.989, Tokens/sec 189.800\
Iter 960: Train loss 0.389, It/sec 0.833, Tokens/sec 241.422\
Iter 970: Train loss 0.567, It/sec 0.976, Tokens/sec 182.547\
Iter 980: Train loss 0.689, It/sec 1.622, Tokens/sec 193.856\
Iter 990: Train loss 0.494, It/sec 1.362, Tokens/sec 259.397\
Iter 1000: Train loss 0.552, It/sec 1.804, Tokens/sec 268.118\
Iter 1000: Val loss 0.343, Val took 10.851s\
Iter 1000: Saved adapter weights to adapters.npz.\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
\
Run 2\
15:24\
15:41\
\
\
chrishayuk$ python lora.py --train --model mistralai/Mistral-7B-Instruct-v0.2 --data /Users/christopherhay/chris-source/chuk-datasets/datasets/calvin_scale/output/calvin_scale_llama/ --batch-size 2 --lora-layers 8 --iters 1000\
Loading pretrained model\
Fetching 11 files: 100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 11/11 [00:00<00:00, 113359.57it/s]\
Total parameters 7242.584M\
Trainable parameters 0.852M\
Loading datasets\
Training\
Iter 1: Val loss 4.810, Val took 9.288s\
Iter 10: Train loss 5.019, It/sec 1.388, Tokens/sec 295.864\
Iter 20: Train loss 4.209, It/sec 2.085, Tokens/sec 273.182\
Iter 30: Train loss 2.589, It/sec 1.511, Tokens/sec 263.322\
Iter 40: Train loss 2.410, It/sec 1.891, Tokens/sec 239.383\
Iter 50: Train loss 1.128, It/sec 0.890, Tokens/sec 281.575\
Iter 60: Train loss 1.573, It/sec 1.635, Tokens/sec 250.130\
Iter 70: Train loss 1.238, It/sec 1.723, Tokens/sec 301.242\
Iter 80: Train loss 0.975, It/sec 1.160, Tokens/sec 254.603\
Iter 90: Train loss 1.064, It/sec 1.426, Tokens/sec 243.246\
Iter 100: Train loss 0.829, It/sec 1.214, Tokens/sec 213.852\
Iter 100: Saved adapter weights to adapters.npz.\
Iter 110: Train loss 0.887, It/sec 1.314, Tokens/sec 251.304\
Iter 120: Train loss 0.719, It/sec 1.195, Tokens/sec 252.724\
Iter 130: Train loss 0.954, It/sec 1.780, Tokens/sec 261.505\
Iter 140: Train loss 0.839, It/sec 1.535, Tokens/sec 259.706\
Iter 150: Train loss 1.103, It/sec 2.762, Tokens/sec 259.634\
Iter 160: Train loss 0.774, It/sec 1.439, Tokens/sec 270.410\
Iter 170: Train loss 0.757, It/sec 1.328, Tokens/sec 245.347\
Iter 180: Train loss 0.903, It/sec 2.052, Tokens/sec 254.678\
Iter 190: Train loss 0.673, It/sec 1.261, Tokens/sec 275.469\
Iter 200: Train loss 0.699, It/sec 1.467, Tokens/sec 227.985\
Iter 200: Val loss 0.485, Val took 12.132s\
Iter 200: Saved adapter weights to adapters.npz.\
Iter 210: Train loss 0.917, It/sec 1.709, Tokens/sec 207.347\
Iter 220: Train loss 1.008, It/sec 2.056, Tokens/sec 192.234\
Iter 230: Train loss 0.657, It/sec 0.979, Tokens/sec 192.144\
Iter 240: Train loss 0.795, It/sec 1.308, Tokens/sec 166.543\
Iter 250: Train loss 0.786, It/sec 1.073, Tokens/sec 152.200\
Iter 260: Train loss 0.624, It/sec 0.764, Tokens/sec 146.912\
Iter 270: Train loss 0.652, It/sec 0.815, Tokens/sec 172.723\
Iter 280: Train loss 0.565, It/sec 0.734, Tokens/sec 159.358\
Iter 290: Train loss 0.697, It/sec 0.896, Tokens/sec 151.026\
Iter 300: Train loss 0.788, It/sec 1.052, Tokens/sec 152.482\
Iter 300: Saved adapter weights to adapters.npz.\
Iter 310: Train loss 0.733, It/sec 0.934, Tokens/sec 197.560\
Iter 320: Train loss 0.549, It/sec 0.834, Tokens/sec 181.657\
Iter 330: Train loss 0.881, It/sec 1.381, Tokens/sec 165.267\
Iter 340: Train loss 0.594, It/sec 1.069, Tokens/sec 209.549\
Iter 350: Train loss 0.832, It/sec 1.523, Tokens/sec 185.053\
Iter 360: Train loss 0.791, It/sec 1.876, Tokens/sec 202.992\
Iter 370: Train loss 0.723, It/sec 1.367, Tokens/sec 196.619\
Iter 380: Train loss 0.434, It/sec 0.844, Tokens/sec 204.578\
Iter 390: Train loss 0.764, It/sec 1.663, Tokens/sec 246.192\
Iter 400: Train loss 0.504, It/sec 0.959, Tokens/sec 207.517\
Iter 400: Val loss 0.436, Val took 12.602s\
Iter 400: Saved adapter weights to adapters.npz.\
Iter 410: Train loss 0.675, It/sec 1.721, Tokens/sec 268.706\
Iter 420: Train loss 0.638, It/sec 1.396, Tokens/sec 262.175\
Iter 430: Train loss 0.639, It/sec 1.574, Tokens/sec 272.155\
Iter 440: Train loss 0.649, It/sec 1.496, Tokens/sec 282.813\
Iter 450: Train loss 0.431, It/sec 1.193, Tokens/sec 316.461\
Iter 460: Train loss 0.687, It/sec 1.959, Tokens/sec 336.489\
Iter 470: Train loss 0.666, It/sec 1.908, Tokens/sec 331.458\
Iter 480: Train loss 0.912, It/sec 2.110, Tokens/sec 262.050\
Iter 490: Train loss 0.832, It/sec 2.239, Tokens/sec 263.787\
Iter 500: Train loss 0.627, It/sec 1.542, Tokens/sec 303.181\
Iter 500: Saved adapter weights to adapters.npz.\
Iter 510: Train loss 0.584, It/sec 1.682, Tokens/sec 298.545\
Iter 520: Train loss 0.819, It/sec 2.153, Tokens/sec 312.633\
Iter 530: Train loss 0.708, It/sec 2.183, Tokens/sec 277.220\
Iter 540: Train loss 0.694, It/sec 2.180, Tokens/sec 277.714\
Iter 550: Train loss 0.616, It/sec 1.627, Tokens/sec 275.523\
Iter 560: Train loss 0.556, It/sec 1.589, Tokens/sec 352.631\
Iter 570: Train loss 0.557, It/sec 1.532, Tokens/sec 262.547\
Iter 580: Train loss 0.666, It/sec 1.839, Tokens/sec 314.032\
Iter 590: Train loss 0.477, It/sec 1.183, Tokens/sec 271.845\
Iter 600: Train loss 0.509, It/sec 1.185, Tokens/sec 278.461\
Iter 600: Val loss 0.395, Val took 10.421s\
Iter 600: Saved adapter weights to adapters.npz.\
Iter 610: Train loss 0.611, It/sec 1.316, Tokens/sec 262.838\
Iter 620: Train loss 0.578, It/sec 1.669, Tokens/sec 283.845\
Iter 630: Train loss 0.527, It/sec 1.547, Tokens/sec 344.765\
Iter 640: Train loss 0.621, It/sec 1.717, Tokens/sec 326.574\
Iter 650: Train loss 0.619, It/sec 1.636, Tokens/sec 308.125\
Iter 660: Train loss 0.535, It/sec 1.550, Tokens/sec 271.665\
Iter 670: Train loss 0.689, It/sec 2.207, Tokens/sec 278.760\
Iter 680: Train loss 0.380, It/sec 1.127, Tokens/sec 276.775\
Iter 690: Train loss 0.734, It/sec 2.557, Tokens/sec 258.269\
Iter 700: Train loss 0.604, It/sec 1.797, Tokens/sec 272.623\
Iter 700: Saved adapter weights to adapters.npz.\
Iter 710: Train loss 0.679, It/sec 2.012, Tokens/sec 251.658\
Iter 720: Train loss 0.514, It/sec 1.327, Tokens/sec 262.521\
Iter 730: Train loss 0.616, It/sec 1.753, Tokens/sec 252.053\
Iter 740: Train loss 0.602, It/sec 1.723, Tokens/sec 258.551\
Iter 750: Train loss 0.499, It/sec 1.500, Tokens/sec 298.051\
Iter 760: Train loss 0.606, It/sec 1.766, Tokens/sec 259.247\
Iter 770: Train loss 0.633, It/sec 1.787, Tokens/sec 261.143\
Iter 780: Train loss 0.689, It/sec 2.092, Tokens/sec 302.041\
Iter 790: Train loss 0.527, It/sec 1.390, Tokens/sec 263.057\
Iter 800: Train loss 0.615, It/sec 1.597, Tokens/sec 265.339\
Iter 800: Val loss 0.349, Val took 10.661s\
Iter 800: Saved adapter weights to adapters.npz.\
Iter 810: Train loss 0.719, It/sec 2.598, Tokens/sec 261.106\
Iter 820: Train loss 0.474, It/sec 1.560, Tokens/sec 272.770\
Iter 830: Train loss 0.534, It/sec 1.576, Tokens/sec 307.529\
Iter 840: Train loss 0.605, It/sec 1.775, Tokens/sec 337.876\
Iter 850: Train loss 0.607, It/sec 2.105, Tokens/sec 263.087\
Iter 860: Train loss 0.516, It/sec 1.520, Tokens/sec 258.992\
Iter 870: Train loss 0.582, It/sec 1.499, Tokens/sec 248.508\
Iter 880: Train loss 0.664, It/sec 2.457, Tokens/sec 239.517\
Iter 890: Train loss 0.507, It/sec 1.667, Tokens/sec 289.936\
Iter 900: Train loss 0.441, It/sec 1.379, Tokens/sec 238.975\
Iter 900: Saved adapter weights to adapters.npz.\
Iter 910: Train loss 0.535, It/sec 1.839, Tokens/sec 240.767\
Iter 920: Train loss 0.495, It/sec 1.474, Tokens/sec 215.330\
Iter 930: Train loss 0.458, It/sec 1.239, Tokens/sec 211.480\
Iter 940: Train loss 0.463, It/sec 0.989, Tokens/sec 223.269\
Iter 950: Train loss 0.367, It/sec 0.978, Tokens/sec 187.668\
Iter 960: Train loss 0.279, It/sec 0.820, Tokens/sec 237.592\
Iter 970: Train loss 0.428, It/sec 1.007, Tokens/sec 188.243\
Iter 980: Train loss 0.527, It/sec 1.262, Tokens/sec 150.840\
Iter 990: Train loss 0.380, It/sec 0.724, Tokens/sec 137.930\
Iter 1000: Train loss 0.405, It/sec 0.862, Tokens/sec 128.159\
Iter 1000: Val loss 0.247, Val took 24.998s\
Iter 1000: Saved adapter weights to adapters.npz.\
}


---------
batching
-------------

chrishayuk$ python lora.py --train --model mistralai/Mistral-7B-Instruct-v0.2 --data /Users/christopherhay/chris-source/chuk-datasets/datasets/calvin_scale/output/calvin_scale_llama/ --batch-size 2 --lora-layers 8 --iters 1000
Loading pretrained model
Fetching 11 files: 100%|██████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 113359.57it/s]
Total parameters 7242.584M
Trainable parameters 0.852M
Loading datasets
Training
encoded
[[1, 1, 2087, 18741, 4060, 1976, 460, 264, 10865, 13892, 28789, 700, 18741, 4060, 28792, 16289, 28793, 5845, 682, 368, 6685, 28705, 28734, 11182, 2300, 8830, 28804, 28792, 28748, 16289, 28793, 10998, 26404, 28742, 2], [1, 1, 2087, 18741, 4060, 995, 460, 396, 16107, 13892, 369, 5312, 1871, 684, 272, 2984, 8830, 7641, 5657, 26364, 700, 18741, 4060, 28792, 16289, 28793, 5660, 5256, 349, 28705, 28740, 8793, 28743, 28708, 28792, 28748, 16289, 28793, 28107, 15257, 28742, 19260, 2]]
length
2
batch
length
43
array([[1, 1, 2087, ..., 0, 0, 0],
       [1, 1, 2087, ..., 15257, 28742, 19260]], dtype=int32) array([[1, 2087, 18741, ..., 0, 0, 0],
       [1, 2087, 18741, ..., 28742, 19260, 2]], dtype=int32) array([35, 43], dtype=int32)
encoded
[[1, 1, 2087, 18741, 4060, 1976, 460, 264, 10865, 13892, 7501, 1871, 684, 272, 2984, 8830, 7641, 5657, 26364, 700, 18741, 4060, 28792, 16289, 28793, 5660, 3296, 349, 387, 28750, 11182, 2984, 8830, 356, 272, 2984, 8830, 2522, 883, 28792, 28748, 16289, 28793, 28107, 15257, 28742, 5228, 26404, 28742, 2], [1, 1, 2087, 18741, 4060, 1976, 460, 264, 10865, 13892, 28789, 700, 18741, 4060, 28792, 16289, 28793, 5660, 5256, 349, 28705, 28734, 11182, 2300, 8830, 28804, 28792, 28748, 16289, 28793, 10998, 26404, 28742, 2]]
length
2
batch
length
50
array([[1, 1, 2087, ..., 5228, 26404, 28742],
       [1, 1, 2087, ..., 0, 0, 0]], dtype=int32) array([[1, 2087, 18741, ..., 26404, 28742, 2],
       [1, 2087, 18741, ..., 0, 0, 0]], dtype=int32) array([50, 34], dtype=int32)
batch
length
43
array([[1, 1, 2087, ..., 10998, 26404, 28742],
       [1, 1, 733, ..., 0, 0, 0]], dtype=int32) array([[1, 2087, 18741, ..., 26404, 28742, 2],
       [1, 733, 16289, ..., 0, 0, 0]], dtype=int32) array([43, 23], dtype=int32)
batch
length
272
array([[1, 1, 733, ..., 0, 0, 0],
       [1, 1, 2087, ..., 10998, 26404, 28742]], dtype=int32) array([[1, 733, 16289, ..., 0, 0, 0],
       [1, 2087, 18741, ..., 26404, 28742, 2]], dtype=int32) array([29, 272], dtype=int32)
batch
length
48
array([[1, 1, 2087, ..., 307, 508, 2834],
       [1, 1, 2087, ..., 0, 0, 0]], dtype=int32) array([[1, 2087, 18741, ..., 508, 2834, 2],
       [1, 2087, 18741, ..., 0, 0, 0]], dtype=int32) array([48, 35], dtype=int32)
batch
length
271
array([[1, 1, 2087, ..., 28758, 626, 723],
       [1, 1, 2087, ..., 0, 0, 0]], dtype=int32) array([[1, 2087, 18741, ..., 626, 723, 2],
       [1, 2087, 18741, ..., 0, 0, 0]], dtype=int32) array([271, 46], dtype=int32)
Iter 1: Val loss 4.809, Val took 9.303s
batch
length
49
array([[1, 1, 2087, ..., 264, 2286, 6895],
       [1, 1, 733, ..., 0, 0, 0]], dtype=int32) array([[1, 2087, 18741, ..., 2286, 6895, 2],
       [1, 733, 16289, ..., 0, 0, 0]], dtype=int32) array([49, 28], dtype=int32)
batch
length
268
array([[1, 1, 2087, ..., 0, 0, 0],
       [1, 1, 2087, ..., 16289, 28793, 22539]], dtype=int32) array([[1, 2087, 18741, ..., 0, 0, 0],
       [1, 2087, 18741, ..., 28793, 22539, 2]], dtype=int32) array([48, 268], dtype=int32)
batch
length
275
array([[1, 1, 733, ..., 0, 0, 0],
       [1, 1, 2087, ..., 5228, 26404, 28742]], dtype=int32) array([[1, 733, 16289, ..., 0, 0, 0],
       [1, 2087, 18741, ..., 26404, 28742, 2]], dtype=int32) array([25, 275], dtype=int32)
batch
length
275
array([[1, 1, 2087, ..., 28793, 1551, 11136],
       [1, 1, 733, ..., 0, 0, 0]], dtype=int32) array([[1, 2087, 18741, ..., 1551, 11136, 2],
       [1, 733, 16289, ..., 0, 0, 0]], dtype=int32) array([275, 25], dtype=int32)