{
  "model": "gemma-3-4b-it-bf16",
  "date": "2025-01-02",
  "researcher": "Christopher Hay",
  "key_discoveries": {
    "1_classification_circuits": {
      "finding": "Gemma has distinct classification circuits for arithmetic vs language",
      "details": [
        "arithmetic_vs_language: Emerges at L0, peaks at L0 (100%)",
        "code_vs_natural: Emerges at L8, peaks at L8 (100%)"
      ],
      "implication": "Task type is detectable from very early layers"
    },
    "2_lookup_table_structure": {
      "finding": "Multiplication table has LOOKUP TABLE structure with perfect commutativity",
      "details": [
        "Commutativity (a*b vs b*a): 0.9993 average across all layers",
        "Early layers (L0-L20): PERFECT commutativity (1.0000)",
        "Late layers (L21-L33): Strong commutativity (0.998+)",
        "Same-product pairs (e.g., 2*6 vs 3*4): 0.9868 similarity at L24",
        "Same-row items (a*_): 0.9788 similarity at L24",
        "Same-column items (_*b): 0.9825 similarity at L24",
        "Columns slightly more organized than rows"
      ],
      "layer_evolution": {
        "L0-L20": "Perfect commutativity (1.0000) - lookup phase",
        "L21": "Commutativity drops to 0.999 - computation begins",
        "L22-L33": "Strong commutativity maintained (0.998) - answer computed"
      },
      "comparison_with_gpt_oss": {
        "similarity": "Both show commutativity baked in from early layers",
        "difference": "Gemma has flatter structure, GPT-OSS has 2D row/column structure",
        "interpretation": "Gemma may use product-indexed lookup, GPT-OSS uses operand-indexed"
      },
      "implication": "Gemma stores multiplication as lookup with commutativity baked in"
    },
    "3_neuron_specialization": {
      "finding": "2,279 of 10,240 neurons (~22%) are highly specialized",
      "top_arithmetic_neurons": [
        {
          "neuron": 7252,
          "separation": 9.64
        },
        {
          "neuron": 5437,
          "separation": 8.15
        },
        {
          "neuron": 4316,
          "separation": 6.52
        }
      ],
      "implication": "There are specific \"arithmetic neurons\" that could be ablated/steered"
    },
    "4_activation_vs_logit": {
      "finding": "Activations contain FAR more information than logits",
      "details": [
        "Mean logit probability for correct answer: 0.375",
        "Mean probe probability from activations: 1.000",
        "Correlation between logit and probe: 0.174"
      ],
      "implication": "Gemma computes correctly but output layer is lossy"
    },
    "5_gemma_exposes_via_activations": {
      "finding": "Original hypothesis CONFIRMED: Gemma exposes computations through activations, not logits",
      "evidence": [
        "Probe accuracy 100% vs model accuracy 37.5% for full answer",
        "Classification circuits readable from hidden states",
        "Full lookup table structure visible in activation space"
      ],
      "implication": "Interpretability research should focus on activations not logits"
    },
    "6_projection_method": {
      "finding": "Standard logit lens FAILS for Gemma, but learned linear probes work perfectly",
      "details": [
        "Standard logit lens (norm(h) @ embed.T) produces garbage at intermediate layers",
        "LogisticRegression probe extracts correct answer with 99.5%+ accuracy from L8 onwards",
        "Answer is encoded as early as L0 (40% probe accuracy), reaches 100% by L16",
        "Training requires only ~50 examples per probe"
      ],
      "method": {
        "tool": "sklearn.linear_model.LogisticRegression",
        "config": "max_iter=1000, C=1.0",
        "input": "Hidden state at last token position",
        "target": "Task-specific labels (e.g., first digit of answer)"
      },
      "implication": "Gemma requires learned projection directions, not vocabulary embedding transpose"
    },
    "7_ablation_robustness": {
      "finding": "Gemma multiplication is EXTREMELY ROBUST to component ablation but NOT layer ablation",
      "details": [
        "Single neuron ablation: 0% accuracy drop for any identified neuron",
        "First 2000 neurons (19.5%) ablated: 0% drop",
        "ALL 8 attention heads at one layer: 0% drop",
        "Ablation verified to modify layer outputs (14.4 difference with 1000 neurons ablated)"
      ],
      "interpretation": {
        "not_a_bug": "Ablation mechanism verified working - outputs change significantly",
        "redundancy": "Within each layer, computation is highly distributed",
        "no_component_failure": "No neurons or attention heads are individually critical",
        "layers_critical": "But ENTIRE LAYERS are critical - see layer ablation below"
      }
    },
    "8_layer_ablation": {
      "finding": "Specific layers are CAUSALLY CRITICAL for multiplication",
      "critical_layers": {
        "L0": "100% drop - embedding processing essential",
        "L4": "100% drop - critical early processing",
        "L1": "90% drop - almost essential",
        "L21": "70% drop - key computation layer",
        "L22": "10% drop - minor contribution"
      },
      "layer_regions": {
        "early_L0_L9": "ESSENTIAL - skipping causes 100% drop",
        "middle_L10_L19": "IMPORTANT - skipping causes 30-100% drop",
        "late_L20_L28": "NEEDED - skipping causes 60-100% drop",
        "final_L29_L33": "DISPENSABLE - skipping causes 0% drop"
      },
      "minimum_circuit": "Cannot use sparse layers - all consecutive layers needed",
      "implication": "Multiplication requires sequential processing through specific layers, not localized circuits"
    },
    "9_activation_steering": {
      "finding": "Activation steering WORKS to modify multiplication behavior",
      "details": [
        "Probe directions achieve 100% accuracy at L16, L20, L24",
        "Negative steering (suppress arithmetic): threshold around -500 strength",
        "At -500: '7 * 8 = ' outputs '7 *' instead of '56'",
        "At -2000: outputs 'The product' instead of '56'",
        "Digit steering partially works: can introduce new digits into output"
      ],
      "effective_layers": [20, 24],
      "threshold": "-500 strength for visible effect",
      "comparison_with_ablation": "Steering works where ablation failed - adding directions is more effective than removing components",
      "implication": "Directions in activation space are causally meaningful even when individual neurons aren't"
    }
  },
  "architectural_insights": {
    "1_distributed_within_layers": "Components (neurons, heads) are massively redundant within each layer",
    "2_sequential_across_layers": "But layer sequence is critical - must process through specific layers",
    "3_early_layers_essential": "L0, L1, L4 are load-bearing for arithmetic",
    "4_late_layers_optional": "Final 5 layers (L29-L33) contribute nothing to multiplication",
    "5_steering_effective": "Activation steering works despite component ablation having no effect"
  },
  "complete_circuit": {
    "finding": "Complete 6-phase multiplication circuit identified",
    "phases": {
      "phase_1_encoding": {
        "layers": "L0-L3",
        "role": "Operand encoding",
        "critical": "L0 (100% drop), L1 (90% drop)"
      },
      "phase_2_recognition": {
        "layers": "L4-L7",
        "role": "Task recognition",
        "critical": "L4 (100% drop)"
      },
      "phase_3_retrieval": {
        "layers": "L8-L16",
        "role": "Lookup table retrieval",
        "evidence": "Perfect commutativity, answer encoded by L16"
      },
      "phase_4_computation": {
        "layers": "L17-L22",
        "role": "Answer crystallization",
        "evidence": "Probe accuracy jumps from 40% to 70% at L21"
      },
      "phase_5_output": {
        "layers": "L23-L28",
        "role": "Output preparation",
        "evidence": "Answer reaches 100% at L26"
      },
      "phase_6_optional": {
        "layers": "L29-L33",
        "role": "General language modeling",
        "evidence": "CAN BE SKIPPED with 0% accuracy loss"
      }
    },
    "attention_patterns": {
      "prompt": "7 * 8 = ",
      "L8_attention_to_op2": 0.224,
      "L8_attention_to_star": 0.243,
      "L22_attention_to_op2": 0.192
    },
    "activation_patching": {
      "source": "7 * 8 = 56",
      "target": "3 * 4 = 12",
      "result": "All layers transfer source answer - answer encoded from earliest layers"
    }
  },
  "universal_pattern": {
    "finding": "Gemma circuit is IDENTICAL to GPT-OSS-20B circuit",
    "comparison": {
      "gpt_oss_layers": 24,
      "gemma_layers": 34,
      "gpt_oss_encoding": "L0-L3",
      "gemma_encoding": "L0-L3",
      "gpt_oss_crystallization": "L19",
      "gemma_crystallization": "L21",
      "gpt_oss_dispensable": "L22-L23",
      "gemma_dispensable": "L29-L33"
    },
    "universal_patterns": [
      "Lookup table mechanism (not algorithm)",
      "6-phase architecture",
      "Early layers critical",
      "Late layers dispensable (~15%)",
      "Component redundancy within layers",
      "Sequential dependency across layers",
      "Steering effective where ablation fails"
    ],
    "implication": "This is a UNIVERSAL transformer pattern for arithmetic"
  },
  "scripts": {
    "gemma_layer_roles.py": "Layer specialization analysis",
    "gemma_lookup_table_analysis.py": "Lookup structure tests",
    "gemma_lookup_evolution.py": "Layer-by-layer evolution",
    "gemma_circuit_via_probes.py": "Circuit identification via probes",
    "gemma_neuron_ablation.py": "Neuron ablation study",
    "gemma_attention_ablation.py": "Attention head ablation",
    "gemma_layer_ablation.py": "Full layer ablation",
    "gemma_activation_steering.py": "Activation steering experiments",
    "gemma_multiplication_circuit.py": "Complete circuit analysis"
  },
  "reproduction": {
    "command_prefix": "uv run python examples/introspection/experiments/model_specific/",
    "model": "mlx-community/gemma-3-4b-it-bf16",
    "framework": "MLX + chuk-lazarus"
  },
  "next_steps": [
    "Test more models (Llama, Qwen, Mistral) to validate universal pattern",
    "Two-digit multiplication circuit analysis",
    "Prune dispensable layers for efficient inference",
    "Addition/subtraction circuit comparison",
    "Training interventions to force algorithmic learning"
  ]
}